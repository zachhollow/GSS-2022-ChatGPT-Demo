{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09f1f2-1396-4d5e-ae35-a6ffa1eb9556",
   "metadata": {},
   "source": [
    "## API Key Setup\n",
    "\n",
    "Follow the steps in the Quickstart Guide to set up your API key.\n",
    "\n",
    "You can also do the following \n",
    "\n",
    "+ Navigate to your project folder `cd ~/Desktop/source/my-project`\n",
    "\n",
    "+ Create an .env file `touch .env`\n",
    "\n",
    "+ Edit the file `nano .env`\n",
    "\n",
    "+ Add your API key as `export OPENAI_API_KEY='your-api-key-here'`\n",
    "\n",
    "+ Hit Ctrl + O to save, then hit enter. Exit using Ctrl + X.   \n",
    "\n",
    "+ Update the gitignore to exclude .env `nano .gitignore`\n",
    "\n",
    "+ Type `.env`\n",
    "\n",
    "+ Save and exit.\n",
    "\n",
    "+ To use the .env variable, type `source .env`\n",
    "\n",
    "## Virtual Environment Setup\n",
    "\n",
    "In Terminal, type:\n",
    "\n",
    "`python -m venv openai-env`\n",
    "\n",
    "## Activation\n",
    "\n",
    "- For Windows, type: `openai-env\\Scripts\\activate`\n",
    "- For MacOS or Unix, type: `source openai-env/bin/activate`\n",
    "\n",
    "If done correctly, you terminal should see `(openai-env)` in your terminal and you can select it as a Kernel moving forward.\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- OpenAI Documentation: https://platform.openai.com/docs/introduction\n",
    "- OpenAI API Reference: https://platform.openai.com/docs/api-reference\n",
    "\n",
    "## Libraries: \n",
    "- `!pip install --upgrade openai`\n",
    "- `!pip install pyreadstat`\n",
    "- `!pip install pandas`\n",
    "- `!pip install urllib3`\n",
    "- `!pip install mysql-connector-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5491bcae-caf9-40b1-a2e1-232bf996672e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File imported successfully as a DataFrame.\n",
      "\n",
      "Here is a preview:\n",
      "\n",
      "   ABANY  ABDEFECT  ABFELEGL  ABHELP1  ABHELP2  ABHELP3  ABHELP4  ABHLTH  \\\n",
      "0    2.0       1.0       NaN      1.0      1.0      1.0      1.0     1.0   \n",
      "1    1.0       1.0       3.0      2.0      2.0      2.0      2.0     1.0   \n",
      "2    NaN       NaN       NaN      1.0      2.0      1.0      1.0     NaN   \n",
      "3    NaN       NaN       1.0      1.0      1.0      1.0      1.0     NaN   \n",
      "4    2.0       1.0       NaN      2.0      2.0      2.0      1.0     1.0   \n",
      "\n",
      "   ABINSPAY  ABMEDGOV1  ...  XMARSEX  XMARSEX1  XMOVIE  XNORCSIZ    YEAR  \\\n",
      "0       1.0        2.0  ...      1.0       1.0     NaN       6.0  2018.0   \n",
      "1       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "2       2.0        1.0  ...      NaN       1.0     2.0       6.0  2018.0   \n",
      "3       1.0        NaN  ...      NaN       NaN     2.0       6.0  2018.0   \n",
      "4       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "\n",
      "   YEARSJOB  YEARSUSA  YEARVAL  YOUSUP  ZODIAC  \n",
      "0       1.0       NaN      NaN    45.0     6.0  \n",
      "1       NaN       NaN      NaN     NaN    11.0  \n",
      "2      15.0       NaN      NaN     3.0     1.0  \n",
      "3      25.0       NaN      NaN    10.0     1.0  \n",
      "4       NaN       NaN      NaN     NaN     4.0  \n",
      "\n",
      "[5 rows x 1065 columns]\n",
      "\n",
      "Number of variables: 1065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pyreadstat\n",
    "import urllib.request\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "\n",
    "# Correct raw URL from GitHub\n",
    "url = \"https://github.com/zachhollow/GSS-ChatGPT-Demo/raw/main/replication_data/GSS2018.sav\"\n",
    "local_file = \"GSS2018.sav\"\n",
    "\n",
    "# Download the file locally\n",
    "urllib.request.urlretrieve(url, local_file)\n",
    "\n",
    "# Use pyreadstat to read the SPSS file\n",
    "df, meta = pyreadstat.read_sav(local_file)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"File imported successfully as a DataFrame.\\n\\nHere is a preview:\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Preview total number of variables \n",
    "count = len(meta.column_names)\n",
    "print(f\"\\nNumber of variables: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7caa46-9d0f-4284-b15d-c12315f4f7af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture capture1\n",
    "#Print all column names and labels saved as meta\n",
    "print(\"Scroll to see the full list of column names and labels:\\n\")\n",
    "for i in range(len(meta.column_names)):\n",
    "    var_name = meta.column_names[i]\n",
    "    var_label = meta.column_labels[i]\n",
    "    print(f\"{var_name}: {var_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf44c3ff-ee8e-47df-83c0-5eafc8e4ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been converted to CSV and saved as GSS2018.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert our SPSS file to a CSV as it's more compatible with Code Interpreter.\n",
    "csv_file_path = \"GSS2018.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"File has been converted to CSV and saved as {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46677f6-d058-429c-91b3-f81480c742d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variable from .env file\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY') # Access your API key using the environment variable\n",
    "\n",
    "client = OpenAI(api_key=api_key)  # Create the OpenAI client instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32915e76-6ec3-4c05-a0d7-bed1307ecc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSS2018.csv successfully uploaded to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Upload our CSV as a file \n",
    "file = client.files.create(\n",
    "                file=open(f\"{csv_file_path}\", \"rb\"),\n",
    "                purpose='assistants',\n",
    "            )\n",
    "print(f\"{csv_file_path} successfully uploaded to OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a257a15-df11-4022-9b91-fd5fcf068c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the FileObject - \n",
      "\n",
      "File Name: file-lyKheVLwRz0j67xwhdCvA4O8\n",
      "\n",
      "File Bytes: 6744448\n",
      "\n",
      "File Created At: 1716594995\n",
      "\n",
      "File Name: GSS2018.csv\n",
      "\n",
      "File Purpose: assistants\n"
     ]
    }
   ],
   "source": [
    "# Access elements of the FileObject\n",
    "print(\"Elements of the FileObject - \\n\")\n",
    "print(f\"File Name: {file.id}\\n\")\n",
    "print(f\"File Bytes: {file.bytes}\\n\")\n",
    "print(f\"File Created At: {file.created_at}\\n\")\n",
    "print(f\"File Name: {file.filename}\\n\")\n",
    "print(f\"File Purpose: {file.purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5466298-1898-4364-844e-f4567d5c1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AssistantEventHandler\n",
    "\n",
    "file = client.files.retrieve(file.id) #Retrieve file\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a research analyst. Summarize data and provide data visualizations.\",\n",
    "    name=\"Research Analyst\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}], \n",
    "    model=\"gpt-4\",\n",
    "    tool_resources={\n",
    "    \"code_interpreter\": {\n",
    "      \"file_ids\": [file.id]\n",
    "    }\n",
    "  }\n",
    ") # Create assistant\n",
    "\n",
    "thread = client.beta.threads.create() # Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c69125c-c8e7-49d2-8f26-9a716e45d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Using GSS2018.csv, generate a cross tab of the following variables, showing percentage. Column variable should be ABSINGLE (Not married). Please export as a CSV and make sure to include the variable names in the CSV.  cross_tab = pd.crosstab(index=data['HRSRELAX’, ‘HELPNOT’, ‘INTECON’, ‘INTENVIR’ ], columns=data['ABSINGLE’, ‘BIGBANG’, ‘CHURHPOW’], normalize='all')  I’d like to have multiple cross-tabulations - one for each column ('HRSRELAX', 'HELPNOT', 'INTECON', 'INTENVIR') against 'ABSINGLE’,  ‘BIGBANG’,  and ‘CHURHPOW’\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant > code_interpreter\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "# Load the data\n",
      "data = pd.read_csv('/mnt/data/file-lyKheVLwRz0j67xwhdCvA4O8')\n",
      "\n",
      "# A preview of the data\n",
      "data.head()\n",
      "\n",
      "Output >\n",
      "\n",
      "   ABANY  ABDEFECT  ABFELEGL  ABHELP1  ABHELP2  ABHELP3  ABHELP4  ABHLTH  \\\n",
      "0    2.0       1.0       NaN      1.0      1.0      1.0      1.0     1.0   \n",
      "1    1.0       1.0       3.0      2.0      2.0      2.0      2.0     1.0   \n",
      "2    NaN       NaN       NaN      1.0      2.0      1.0      1.0     NaN   \n",
      "3    NaN       NaN       1.0      1.0      1.0      1.0      1.0     NaN   \n",
      "4    2.0       1.0       NaN      2.0      2.0      2.0      1.0     1.0   \n",
      "\n",
      "   ABINSPAY  ABMEDGOV1  ...  XMARSEX  XMARSEX1  XMOVIE  XNORCSIZ    YEAR  \\\n",
      "0       1.0        2.0  ...      1.0       1.0     NaN       6.0  2018.0   \n",
      "1       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "2       2.0        1.0  ...      NaN       1.0     2.0       6.0  2018.0   \n",
      "3       1.0        NaN  ...      NaN       NaN     2.0       6.0  2018.0   \n",
      "4       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "\n",
      "   YEARSJOB  YEARSUSA  YEARVAL  YOUSUP  ZODIAC  \n",
      "0       1.0       NaN      NaN    45.0     6.0  \n",
      "1       NaN       NaN      NaN     NaN    11.0  \n",
      "2      15.0       NaN      NaN     3.0     1.0  \n",
      "3      25.0       NaN      NaN    10.0     1.0  \n",
      "4       NaN       NaN      NaN     NaN     4.0  \n",
      "\n",
      "[5 rows x 1065 columns]\n",
      "\n",
      "Assistant > The data consists of 1065 columns. We are interested in 'HRSRELAX', 'HELPNOT', 'INTECON', 'INTENVIR', 'ABSINGLE', 'BIGBANG', and 'CHURHPOW' columns for our cross-tabulations. \n",
      "\n",
      "Let's proceed to create these cross-tabulations as per your request.# Variables\n",
      "index_vars = ['HRSRELAX', 'HELPNOT', 'INTECON', 'INTENVIR']\n",
      "column_vars = ['ABSINGLE', 'BIGBANG', 'CHURHPOW']\n",
      "\n",
      "# Empty dict to store all the cross-tabulation dataframes\n",
      "cross_tabs_dict = {}\n",
      "\n",
      "# Loop over each index and column variable to calculate the cross-tabulation\n",
      "for index_var in index_vars:\n",
      "    for column_var in column_vars:\n",
      "        cross_tabs_dict[(index_var, column_var)] = pd.crosstab(index=data[index_var], columns=data[column_var], normalize='all')\n",
      "\n",
      "cross_tabs_dict\n",
      "\n",
      "Output >\n",
      "\n",
      "{('HRSRELAX',\n",
      "  'ABSINGLE'): ABSINGLE       1.0       2.0\n",
      " HRSRELAX                    \n",
      " 0.0       0.031042  0.032151\n",
      " 1.0       0.047672  0.054324\n",
      " 2.0       0.089800  0.103104\n",
      " 3.0       0.073171  0.074279\n",
      " 4.0       0.089800  0.104213\n",
      " 5.0       0.063193  0.065410\n",
      " 6.0       0.034368  0.028825\n",
      " 7.0       0.008869  0.004435\n",
      " 8.0       0.024390  0.025499\n",
      " 9.0       0.000000  0.003326\n",
      " 10.0      0.007761  0.007761\n",
      " 11.0      0.001109  0.001109\n",
      " 12.0      0.001109  0.007761\n",
      " 14.0      0.002217  0.002217\n",
      " 15.0      0.001109  0.001109\n",
      " 16.0      0.002217  0.000000\n",
      " 17.0      0.001109  0.000000\n",
      " 18.0      0.000000  0.001109\n",
      " 20.0      0.001109  0.001109\n",
      " 21.0      0.001109  0.000000\n",
      " 24.0      0.001109  0.000000,\n",
      " ('HRSRELAX',\n",
      "  'BIGBANG'): BIGBANG        1.0       2.0\n",
      " HRSRELAX                    \n",
      " 0.0       0.017647  0.058824\n",
      " 1.0       0.058824  0.047059\n",
      " 2.0       0.105882  0.141176\n",
      " 3.0       0.070588  0.082353\n",
      " 4.0       0.141176  0.052941\n",
      " 5.0       0.088235  0.023529\n",
      " 6.0       0.017647  0.011765\n",
      " 7.0       0.005882  0.000000\n",
      " 8.0       0.023529  0.011765\n",
      " 9.0       0.000000  0.005882\n",
      " 10.0      0.017647  0.000000\n",
      " 15.0      0.005882  0.000000\n",
      " 16.0      0.005882  0.000000\n",
      " 20.0      0.000000  0.005882,\n",
      " ('HRSRELAX',\n",
      "  'CHURHPOW'): CHURHPOW       1.0       2.0       3.0       4.0       5.0\n",
      " HRSRELAX                                                  \n",
      " 0.0       0.004412  0.014706  0.036765  0.013235  0.001471\n",
      " 1.0       0.014706  0.038235  0.045588  0.020588  0.000000\n",
      " 2.0       0.014706  0.066176  0.089706  0.041176  0.001471\n",
      " 3.0       0.007353  0.048529  0.051471  0.036765  0.002941\n",
      " 4.0       0.010294  0.060294  0.088235  0.027941  0.001471\n",
      " 5.0       0.002941  0.020588  0.051471  0.022059  0.000000\n",
      " 6.0       0.007353  0.027941  0.025000  0.005882  0.001471\n",
      " 7.0       0.002941  0.001471  0.005882  0.001471  0.000000\n",
      " 8.0       0.013235  0.007353  0.017647  0.011765  0.000000\n",
      " 9.0       0.000000  0.000000  0.001471  0.001471  0.000000\n",
      " 10.0      0.000000  0.005882  0.005882  0.000000  0.000000\n",
      " 11.0      0.000000  0.000000  0.001471  0.000000  0.000000\n",
      " 12.0      0.001471  0.001471  0.001471  0.001471  0.000000\n",
      " 14.0      0.000000  0.000000  0.000000  0.002941  0.000000\n",
      " 15.0      0.000000  0.002941  0.000000  0.000000  0.000000\n",
      " 16.0      0.000000  0.000000  0.001471  0.000000  0.000000\n",
      " 20.0      0.002941  0.001471  0.001471  0.000000  0.000000,\n",
      " ('HELPNOT',\n",
      "  'ABSINGLE'): ABSINGLE       1.0       2.0\n",
      " HELPNOT                     \n",
      " 1.0       0.066576  0.095109\n",
      " 2.0       0.092391  0.069293\n",
      " 3.0       0.209239  0.220109\n",
      " 4.0       0.050272  0.081522\n",
      " 5.0       0.044837  0.070652,\n",
      " ('HELPNOT',\n",
      "  'BIGBANG'): Empty DataFrame\n",
      " Columns: []\n",
      " Index: [],\n",
      " ('HELPNOT',\n",
      "  'CHURHPOW'): CHURHPOW       1.0       2.0       3.0       4.0       5.0\n",
      " HELPNOT                                                   \n",
      " 1.0       0.011396  0.051282  0.045584  0.042735  0.002849\n",
      " 2.0       0.005698  0.045584  0.071225  0.022792  0.000000\n",
      " 3.0       0.022792  0.139601  0.193732  0.076923  0.000000\n",
      " 4.0       0.011396  0.031339  0.085470  0.005698  0.000000\n",
      " 5.0       0.005698  0.045584  0.045584  0.037037  0.000000,\n",
      " ('INTECON',\n",
      "  'ABSINGLE'): ABSINGLE       1.0       2.0\n",
      " INTECON                     \n",
      " 1.0       0.199734  0.203728\n",
      " 2.0       0.210386  0.230360\n",
      " 3.0       0.055925  0.099867,\n",
      " ('INTECON',\n",
      "  'BIGBANG'): BIGBANG       1.0       2.0\n",
      " INTECON                    \n",
      " 1.0      0.197324  0.173913\n",
      " 2.0      0.244147  0.230769\n",
      " 3.0      0.070234  0.083612,\n",
      " ('INTECON',\n",
      "  'CHURHPOW'): CHURHPOW       1.0       2.0       3.0       4.0       5.0\n",
      " INTECON                                                   \n",
      " 1.0       0.030493  0.120179  0.172197  0.071749  0.007175\n",
      " 2.0       0.030493  0.127354  0.203587  0.086996  0.003587\n",
      " 3.0       0.013453  0.038565  0.055605  0.034978  0.003587,\n",
      " ('INTENVIR',\n",
      "  'ABSINGLE'): ABSINGLE       1.0       2.0\n",
      " INTENVIR                    \n",
      " 1.0       0.245007  0.207723\n",
      " 2.0       0.194407  0.245007\n",
      " 3.0       0.026631  0.081225,\n",
      " ('INTENVIR',\n",
      "  'BIGBANG'): BIGBANG        1.0       2.0\n",
      " INTENVIR                    \n",
      " 1.0       0.244147  0.234114\n",
      " 2.0       0.214047  0.204013\n",
      " 3.0       0.053512  0.050167,\n",
      " ('INTENVIR',\n",
      "  'CHURHPOW'): CHURHPOW       1.0       2.0       3.0       4.0       5.0\n",
      " INTENVIR                                                  \n",
      " 1.0       0.043986  0.146320  0.179533  0.077199  0.008079\n",
      " 2.0       0.021544  0.114901  0.210054  0.089767  0.003591\n",
      " 3.0       0.008977  0.025135  0.042190  0.026032  0.002693}\n",
      "\n",
      "Assistant > I've computed cross-tabulations for your provided combinations. Here are few of them:\n",
      "\n",
      "Cross-tabulation for ('HRSRELAX', 'ABSINGLE'):\n",
      "```\n",
      "ABSINGLE       1.0       2.0\n",
      "HRSRELAX                    \n",
      "0.0       0.031042  0.032151\n",
      "1.0       0.047672  0.054324\n",
      "2.0       0.089800  0.103104\n",
      "3.0       0.073171  0.074279\n",
      "4.0       0.089800  0.104213\n",
      "5.0       0.063193  0.065410\n",
      "6.0       0.034368  0.028825\n",
      "7.0       0.008869  0.004435\n",
      "8.0       0.024390  0.025499\n",
      "9.0       0.000000  0.003326\n",
      "10.0      0.007761  0.007761\n",
      "11.0      0.001109  0.001109\n",
      "12.0      0.001109  0.007761\n",
      "```\n",
      "Cross-tabulation for ('INTENVIR', 'CHURHPOW'):\n",
      "```\n",
      "CHURHPOW       1.0       2.0       3.0       4.0       5.0\n",
      "INTENVIR                                                  \n",
      "1.0       0.043986  0.146320  0.179533  0.077199  0.008079\n",
      "2.0       0.021544  0.114901  0.210054  0.089767  0.003591\n",
      "3.0       0.008977  0.025135  0.042190  0.026032  0.002693\n",
      "```\n",
      "Let's save all these cross-tabulations as individual CSV files to /mnt/data.# Save the Crosstab tables as CSV files\n",
      "for key, cross_tab_df in cross_tabs_dict.items():\n",
      "    file_name = f'/mnt/data/cross_tabulation_{key[0]}_{key[1]}.csv'\n",
      "    cross_tab_df.to_csv(file_name)\n",
      "\n",
      "Output >\n",
      "\n",
      "\n",
      "\n",
      "Assistant > I have saved all the cross-tabulated tables as individual CSV files. You can download them using the following links:\n",
      "\n",
      "1. ['HRSRELAX', 'ABSINGLE'](sandbox:/mnt/data/cross_tabulation_HRSRELAX_ABSINGLE.csv)\n",
      "2. ['HRSRELAX', 'BIGBANG'](sandbox:/mnt/data/cross_tabulation_HRSRELAX_BIGBANG.csv)\n",
      "3. ['HRSRELAX', 'CHURHPOW'](sandbox:/mnt/data/cross_tabulation_HRSRELAX_CHURHPOW.csv)\n",
      "4. ['HELPNOT', 'ABSINGLE'](sandbox:/mnt/data/cross_tabulation_HELPNOT_ABSINGLE.csv)\n",
      "5. ['HELPNOT', 'BIGBANG'](sandbox:/mnt/data/cross_tabulation_HELPNOT_BIGBANG.csv)\n",
      "6. ['HELPNOT', 'CHURHPOW'](sandbox:/mnt/data/cross_tabulation_HELPNOT_CHURHPOW.csv)\n",
      "7. ['INTECON', 'ABSINGLE'](sandbox:/mnt/data/cross_tabulation_INTECON_ABSINGLE.csv)\n",
      "8. ['INTECON', 'BIGBANG'](sandbox:/mnt/data/cross_tabulation_INTECON_BIGBANG.csv)\n",
      "9. ['INTECON', 'CHURHPOW'](sandbox:/mnt/data/cross_tabulation_INTECON_CHURHPOW.csv)\n",
      "10. ['INTENVIR', 'ABSINGLE'](sandbox:/mnt/data/cross_tabulation_INTENVIR_ABSINGLE.csv)\n",
      "11. ['INTENVIR', 'BIGBANG'](sandbox:/mnt/data/cross_tabulation_INTENVIR_BIGBANG.csv)\n",
      "12. ['INTENVIR', 'CHURHPOW'](sandbox:/mnt/data/cross_tabulation_INTENVIR_CHURHPOW.csv)\n",
      "\n",
      "Please click on the links to download the files.None"
     ]
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=input(\"\\nYou: \") \n",
    ") # Add input to thread \n",
    "  # NOTE: this code only handles one input at a time\n",
    "\n",
    "# Create a EventHandler class to define how we want to handle the events in the response stream\n",
    "# Use the `stream` SDK helper with the `EventHandler` class to create the Run and stream the response\n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nAssistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nAssistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\nOutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)\n",
    " \n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Zach. The user has a background in data analytics and machine learning using Python.\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34e4527-4b5d-403a-a372-642fa011a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files available via the OpenAI API\n",
    "file_list_response = client.files.list()\n",
    "\n",
    "# Retrieve the data attribute from the SyncPage response\n",
    "file_objects = file_list_response.data\n",
    "\n",
    "# Access the first FileObject and retrieve its id attribute for a quick preview\n",
    "first_file_object_id = file_objects[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab91f0a-051f-4b64-9fd2-c59b73b30de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_crosstab = first_file_object_id\n",
    "\n",
    "def download_file(summary_crosstab):\n",
    "    # Retrieve the file\n",
    "    file = client.files.retrieve(summary_crosstab)\n",
    "    # Get the content\n",
    "    content_response = client.files.content(summary_crosstab)\n",
    "    \n",
    "    # Extract the binary content from the response\n",
    "    content = content_response.read()\n",
    "    \n",
    "    # Write the content to a file\n",
    "    with open('cross_tab_summary.csv', 'wb') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Call the function\n",
    "download_file(summary_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288e0fa9-7740-4dbc-b45a-0cabe5da1477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   INTENVIR       1.0       2.0       3.0       4.0       5.0\n",
      "0       1.0  0.043986  0.146320  0.179533  0.077199  0.008079\n",
      "1       2.0  0.021544  0.114901  0.210054  0.089767  0.003591\n",
      "2       3.0  0.008977  0.025135  0.042190  0.026032  0.002693\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('cross_tab_summary.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai-env)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
