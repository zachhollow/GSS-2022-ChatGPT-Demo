{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09f1f2-1396-4d5e-ae35-a6ffa1eb9556",
   "metadata": {},
   "source": [
    "## API Key Setup\n",
    "\n",
    "Follow the steps in the Quickstart Guide to set up your API key.\n",
    "\n",
    "You can also do the following \n",
    "\n",
    "+ Navigate to your project folder `cd ~/Desktop/source/my-project`\n",
    "\n",
    "+ Create an .env file `touch .env`\n",
    "\n",
    "+ Edit the file `nano .env`\n",
    "\n",
    "+ Add your API key as `export OPENAI_API_KEY='your-api-key-here'`\n",
    "\n",
    "+ Hit Ctrl + O to save, then hit enter. Exit using Ctrl + X.   \n",
    "\n",
    "+ Update the gitignore to exclude .env `nano .gitignore`\n",
    "\n",
    "+ Type `.env`\n",
    "\n",
    "+ Save and exit.\n",
    "\n",
    "+ To use the .env variable, type `source .env`\n",
    "\n",
    "## Virtual Environment Setup\n",
    "\n",
    "In Terminal, type:\n",
    "\n",
    "`python -m venv openai-env`\n",
    "\n",
    "## Activation\n",
    "\n",
    "- For Windows, type: `openai-env\\Scripts\\activate`\n",
    "- For MacOS or Unix, type: `source openai-env/bin/activate`\n",
    "\n",
    "If done correctly, you terminal should see `(openai-env)` in your terminal and you can select it as a Kernel moving forward.\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- OpenAI Documentation: https://platform.openai.com/docs/introduction\n",
    "- OpenAI API Reference: https://platform.openai.com/docs/api-reference\n",
    "\n",
    "## Libraries: \n",
    "- `!pip install --upgrade openai`\n",
    "- `!pip install pyreadstat`\n",
    "- `!pip install pandas`\n",
    "- `!pip install urllib3`\n",
    "- `!pip install mysql-connector-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5491bcae-caf9-40b1-a2e1-232bf996672e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File imported successfully as a DataFrame.\n",
      "\n",
      "Here is a preview:\n",
      "\n",
      "   ABANY  ABDEFECT  ABFELEGL  ABHELP1  ABHELP2  ABHELP3  ABHELP4  ABHLTH  \\\n",
      "0    2.0       1.0       NaN      1.0      1.0      1.0      1.0     1.0   \n",
      "1    1.0       1.0       3.0      2.0      2.0      2.0      2.0     1.0   \n",
      "2    NaN       NaN       NaN      1.0      2.0      1.0      1.0     NaN   \n",
      "3    NaN       NaN       1.0      1.0      1.0      1.0      1.0     NaN   \n",
      "4    2.0       1.0       NaN      2.0      2.0      2.0      1.0     1.0   \n",
      "\n",
      "   ABINSPAY  ABMEDGOV1  ...  XMARSEX  XMARSEX1  XMOVIE  XNORCSIZ    YEAR  \\\n",
      "0       1.0        2.0  ...      1.0       1.0     NaN       6.0  2018.0   \n",
      "1       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "2       2.0        1.0  ...      NaN       1.0     2.0       6.0  2018.0   \n",
      "3       1.0        NaN  ...      NaN       NaN     2.0       6.0  2018.0   \n",
      "4       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "\n",
      "   YEARSJOB  YEARSUSA  YEARVAL  YOUSUP  ZODIAC  \n",
      "0       1.0       NaN      NaN    45.0     6.0  \n",
      "1       NaN       NaN      NaN     NaN    11.0  \n",
      "2      15.0       NaN      NaN     3.0     1.0  \n",
      "3      25.0       NaN      NaN    10.0     1.0  \n",
      "4       NaN       NaN      NaN     NaN     4.0  \n",
      "\n",
      "[5 rows x 1065 columns]\n",
      "\n",
      "Number of variables: 1065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pyreadstat\n",
    "import urllib.request\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "\n",
    "# Correct raw URL from GitHub\n",
    "url = \"https://github.com/zachhollow/GSS-ChatGPT-Demo/raw/main/replication_data/GSS2018.sav\"\n",
    "local_file = \"GSS2018.sav\"\n",
    "\n",
    "# Download the file locally\n",
    "urllib.request.urlretrieve(url, local_file)\n",
    "\n",
    "# Use pyreadstat to read the SPSS file\n",
    "df, meta = pyreadstat.read_sav(local_file)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"File imported successfully as a DataFrame.\\n\\nHere is a preview:\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Preview total number of variables \n",
    "count = len(meta.column_names)\n",
    "print(f\"\\nNumber of variables: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7caa46-9d0f-4284-b15d-c12315f4f7af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture capture1\n",
    "#Print all column names and labels saved as meta\n",
    "print(\"Scroll to see the full list of column names and labels:\\n\")\n",
    "for i in range(len(meta.column_names)):\n",
    "    var_name = meta.column_names[i]\n",
    "    var_label = meta.column_labels[i]\n",
    "    print(f\"{var_name}: {var_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf44c3ff-ee8e-47df-83c0-5eafc8e4ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been converted to CSV and saved as GSS2018.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert our SPSS file to a CSV as it's more compatible with Code Interpreter.\n",
    "csv_file_path = \"GSS2018.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"File has been converted to CSV and saved as {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46677f6-d058-429c-91b3-f81480c742d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variable from .env file\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY') # Access your API key using the environment variable\n",
    "\n",
    "client = OpenAI(api_key=api_key)  # Create the OpenAI client instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32915e76-6ec3-4c05-a0d7-bed1307ecc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSS2018.csv successfully uploaded to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Upload our CSV as a file \n",
    "file = client.files.create(\n",
    "                file=open(f\"{csv_file_path}\", \"rb\"),\n",
    "                purpose='assistants',\n",
    "            )\n",
    "print(f\"{csv_file_path} successfully uploaded to OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a257a15-df11-4022-9b91-fd5fcf068c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the FileObject - \n",
      "\n",
      "File Name: file-Aimi1qoCDhnr9YISzRo53Oq7\n",
      "\n",
      "File Bytes: 6744448\n",
      "\n",
      "File Created At: 1716595786\n",
      "\n",
      "File Name: GSS2018.csv\n",
      "\n",
      "File Purpose: assistants\n"
     ]
    }
   ],
   "source": [
    "# Access elements of the FileObject\n",
    "print(\"Elements of the FileObject - \\n\")\n",
    "print(f\"File Name: {file.id}\\n\")\n",
    "print(f\"File Bytes: {file.bytes}\\n\")\n",
    "print(f\"File Created At: {file.created_at}\\n\")\n",
    "print(f\"File Name: {file.filename}\\n\")\n",
    "print(f\"File Purpose: {file.purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5466298-1898-4364-844e-f4567d5c1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AssistantEventHandler\n",
    "\n",
    "file = client.files.retrieve(file.id) #Retrieve file\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a research analyst. Summarize data and provide data visualizations.\",\n",
    "    name=\"Research Analyst\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}], \n",
    "    model=\"gpt-4\",\n",
    "    tool_resources={\n",
    "    \"code_interpreter\": {\n",
    "      \"file_ids\": [file.id]\n",
    "    }\n",
    "  }\n",
    ") # Create assistant\n",
    "\n",
    "thread = client.beta.threads.create() # Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c69125c-c8e7-49d2-8f26-9a716e45d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Can you run regression to see whether INTECON (y) is casually driven by ABSINGLE (x). Maybe include CHURHPOW and HELPNOT as control variables. Please drop all missing values and provide data visualizations, tables or other files as you see necessary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant > Absolutely, Zach, I can help with that. Let me begin by loading your data and investigating its structure. Once I have a good sense of the dataset, I'll proceed to implement the linear regression and control for 'CHURHPOW' and 'HELPNOT'.\n",
      "Assistant > code_interpreter\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_csv('/mnt/data/file-Aimi1qoCDhnr9YISzRo53Oq7')\n",
      "\n",
      "# Display the first few rows of the dataset\n",
      "data.head()\n",
      "\n",
      "Output >\n",
      "\n",
      "   ABANY  ABDEFECT  ABFELEGL  ABHELP1  ABHELP2  ABHELP3  ABHELP4  ABHLTH  \\\n",
      "0    2.0       1.0       NaN      1.0      1.0      1.0      1.0     1.0   \n",
      "1    1.0       1.0       3.0      2.0      2.0      2.0      2.0     1.0   \n",
      "2    NaN       NaN       NaN      1.0      2.0      1.0      1.0     NaN   \n",
      "3    NaN       NaN       1.0      1.0      1.0      1.0      1.0     NaN   \n",
      "4    2.0       1.0       NaN      2.0      2.0      2.0      1.0     1.0   \n",
      "\n",
      "   ABINSPAY  ABMEDGOV1  ...  XMARSEX  XMARSEX1  XMOVIE  XNORCSIZ    YEAR  \\\n",
      "0       1.0        2.0  ...      1.0       1.0     NaN       6.0  2018.0   \n",
      "1       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "2       2.0        1.0  ...      NaN       1.0     2.0       6.0  2018.0   \n",
      "3       1.0        NaN  ...      NaN       NaN     2.0       6.0  2018.0   \n",
      "4       2.0        NaN  ...      1.0       NaN     2.0       6.0  2018.0   \n",
      "\n",
      "   YEARSJOB  YEARSUSA  YEARVAL  YOUSUP  ZODIAC  \n",
      "0       1.0       NaN      NaN    45.0     6.0  \n",
      "1       NaN       NaN      NaN     NaN    11.0  \n",
      "2      15.0       NaN      NaN     3.0     1.0  \n",
      "3      25.0       NaN      NaN    10.0     1.0  \n",
      "4       NaN       NaN      NaN     NaN     4.0  \n",
      "\n",
      "[5 rows x 1065 columns]\n",
      "\n",
      "Assistant > The dataset contains 1065 columns. Among those, we're primarily interested in the 'INTECON', 'ABSINGLE', 'CHURHPOW', and 'HELPNOT' variables. Let's examine these columns in more detail. We'll check their missing values and plot some basic univariate histograms to understand their distributions. Then, we'll proceed to build the linear regression model you requested.import matplotlib.pyplot as plt\n",
      "\n",
      "# Select the columns we are interested in and drop missing values\n",
      "data_subset = data[['INTECON', 'ABSINGLE', 'CHURHPOW', 'HELPNOT']].dropna()\n",
      "\n",
      "# Display summary statistics of the data\n",
      "summary_stats = data_subset.describe()\n",
      "\n",
      "# Plot histograms of the variables\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n",
      "\n",
      "axes[0, 0].hist(data_subset['INTECON'], bins=30, color='lightblue', edgecolor='black')\n",
      "axes[0, 0].set_title('INTECON')\n",
      "\n",
      "axes[0, 1].hist(data_subset['ABSINGLE'], bins=30, color='lightblue', edgecolor='black')\n",
      "axes[0, 1].set_title('ABSINGLE')\n",
      "\n",
      "axes[1, 0].hist(data_subset['CHURHPOW'], bins=30, color='lightblue', edgecolor='black')\n",
      "axes[1, 0].set_title('CHURHPOW')\n",
      "\n",
      "axes[1, 1].hist(data_subset['HELPNOT'], bins=30, color='lightblue', edgecolor='black')\n",
      "axes[1, 1].set_title('HELPNOT')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "summary_stats\n",
      "\n",
      "Output >\n",
      "\n",
      "Assistant > It appears that the variables 'INTECON', 'ABSINGLE', 'CHURHPOW', and 'HELPNOT' have missing values in all of their rows. I suspect there may have been a mistake. Could you kindly confirm that these are the right variables from your dataset for the regression model?"
     ]
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=input(\"\\nYou: \") \n",
    ") # Add input to thread \n",
    "  # NOTE: this code only handles one input at a time\n",
    "\n",
    "# Create a EventHandler class to define how we want to handle the events in the response stream\n",
    "# Use the `stream` SDK helper with the `EventHandler` class to create the Run and stream the response\n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nAssistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nAssistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\nOutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)\n",
    " \n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Zach. The user has a background in data analytics and machine learning using Python.\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40130a3-4c4b-435d-a905-18595cc23123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai-env)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
